{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea00fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b785b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\D\n",
      "[nltk_data]     Navya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\D\n",
      "[nltk_data]     Navya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\D\n",
      "[nltk_data]     Navya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\D\n",
      "[nltk_data]     Navya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\D\n",
      "[nltk_data]     Navya\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 3: Stemming and Lemmatization for Text Preprocessing\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d81691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 100\n",
      "Total labels loaded: 100\n",
      "\n",
      "Sample document:\n",
      "Text: oooh, sunshine! A patch of sunshine! And it will be gone by the time I leave work and replaced with rain.  /vent\n",
      "Label: neutral\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the dataset into a Pandas DataFrame and extract the text and label column\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('cleaned_data.csv', index_col=0)\n",
    "\n",
    "# Extract text and label columns\n",
    "texts = df['review_text'].tolist()\n",
    "labels = df['sentiment'].tolist()\n",
    "\n",
    "print(f\"Total documents loaded: {len(texts)}\")\n",
    "print(f\"Total labels loaded: {len(labels)}\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(f\"Text: {texts[0]}\")\n",
    "print(f\"Label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822eda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents tokenized: 100\n",
      "\n",
      "Example - First document:\n",
      "Original: oooh, sunshine! A patch of sunshine! And it will be gone by the time I leave work and replaced with ...\n",
      "Tokens: ['oooh', ',', 'sunshine', '!', 'A', 'patch', 'of', 'sunshine', '!', 'And', 'it', 'will', 'be', 'gone', 'by']\n",
      "Total tokens in first doc: 26\n"
     ]
    }
   ],
   "source": [
    "# 2. Perform tokenization on all documents and store the tokens\n",
    "\n",
    "# Tokenize all documents\n",
    "tokenized_docs = []\n",
    "for text in texts:\n",
    "    tokens = word_tokenize(text)\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "print(f\"Total documents tokenized: {len(tokenized_docs)}\")\n",
    "print(f\"\\nExample - First document:\")\n",
    "print(f\"Original: {texts[0][:100]}...\")\n",
    "print(f\"Tokens: {tokenized_docs[0][:15]}\")\n",
    "print(f\"Total tokens in first doc: {len(tokenized_docs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "220a1a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case folding applied to all 100 documents\n",
      "\n",
      "Example - First document after case folding:\n",
      "Before: ['oooh', ',', 'sunshine', '!', 'A', 'patch', 'of', 'sunshine', '!', 'And']\n",
      "After: ['oooh', ',', 'sunshine', '!', 'a', 'patch', 'of', 'sunshine', '!', 'and']\n"
     ]
    }
   ],
   "source": [
    "# 3. Apply case folding by converting all tokens to lowercase\n",
    "\n",
    "# Convert all tokens to lowercase\n",
    "lowercase_docs = []\n",
    "for tokens in tokenized_docs:\n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "    lowercase_docs.append(lowercase_tokens)\n",
    "\n",
    "print(f\"Case folding applied to all {len(lowercase_docs)} documents\")\n",
    "print(f\"\\nExample - First document after case folding:\")\n",
    "print(f\"Before: {tokenized_docs[0][:10]}\")\n",
    "print(f\"After: {lowercase_docs[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop-words loaded: 198\n",
      "\n",
      "Example - First document after stop-word removal:\n",
      "Before (lowercase): ['oooh', ',', 'sunshine', '!', 'a', 'patch', 'of', 'sunshine', '!', 'and', 'it', 'will', 'be', 'gone', 'by', 'the', 'time', 'i', 'leave', 'work', 'and', 'replaced', 'with', 'rain', '.']\n",
      "After (filtered): ['oooh', 'sunshine', 'patch', 'sunshine', 'gone', 'time', 'leave', 'work', 'replaced', 'rain']\n",
      "Tokens before: 26, Tokens after: 10\n"
     ]
    }
   ],
   "source": [
    "# 4. Remove stop-words from the tokenized documents\n",
    "\n",
    "# Load English stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop-words and non-alphabetic tokens\n",
    "filtered_docs = []\n",
    "for tokens in lowercase_docs:\n",
    "    filtered_tokens = [token for token in tokens \n",
    "                      if token not in stop_words and token.isalpha()]\n",
    "    filtered_docs.append(filtered_tokens)\n",
    "\n",
    "print(f\"Stop-words loaded: {len(stop_words)}\")\n",
    "print(f\"\\nExample - First document after stop-word removal:\")\n",
    "print(f\"Before (lowercase): {lowercase_docs[0][:25]}\")\n",
    "print(f\"After (filtered): {filtered_docs[0][:25]}\")\n",
    "print(f\"Tokens before: {len(lowercase_docs[0])}, Tokens after: {len(filtered_docs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03afb4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming applied to all 100 documents\n",
      "\n",
      "Example - Stemming transformations:\n",
      "  sunshine                  -> sunshin\n",
      "  sunshine                  -> sunshin\n",
      "  leave                     -> leav\n",
      "  replaced                  -> replac\n"
     ]
    }
   ],
   "source": [
    "# 5. Apply stemming to the filtered tokens\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to all filtered documents\n",
    "stemmed_docs = []\n",
    "for tokens in filtered_docs:\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    stemmed_docs.append(stemmed_tokens)\n",
    "\n",
    "print(f\"Stemming applied to all {len(stemmed_docs)} documents\")\n",
    "print(f\"\\nExample - Stemming transformations:\")\n",
    "sample_tokens = filtered_docs[0][:10]\n",
    "sample_stemmed = stemmed_docs[0][:10]\n",
    "for original, stemmed in zip(sample_tokens, sample_stemmed):\n",
    "    if original != stemmed:\n",
    "        print(f\"  {original:25} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12e120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text column created\n",
      "\n",
      "Example - First document:\n",
      "Original: oooh, sunshine! A patch of sunshine! And it will be gone by the time I leave work and replaced with ...\n",
      "Stemmed: oooh sunshin patch sunshin gone time leav work replac rain...\n"
     ]
    }
   ],
   "source": [
    "# 6. Create a new column with final preprocessed text after stemming\n",
    "\n",
    "# Join stemmed tokens back into text\n",
    "df['stemmed_text'] = [' '.join(tokens) for tokens in stemmed_docs]\n",
    "\n",
    "print(f\"Stemmed text column created\")\n",
    "print(f\"\\nExample - First document:\")\n",
    "print(f\"Original: {texts[0][:100]}...\")\n",
    "print(f\"Stemmed: {df['stemmed_text'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0034108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stemmed tokens (with repetition): 701\n",
      "Stemmed vocabulary size: 487\n",
      "\n",
      "Sample words from stemmed vocabulary: ['abil', 'absout', 'accept', 'account', 'act', 'ad', 'adapt', 'adob', 'age', 'agenc', 'ago', 'agre', 'ah', 'ahem', 'ahhh', 'alreadi', 'alright', 'also', 'alway', 'anim']\n"
     ]
    }
   ],
   "source": [
    "# 7. Construct the Bag-of-Words vocabulary using stemmed text\n",
    "\n",
    "\n",
    "# Collect all stemmed tokens\n",
    "all_stemmed_tokens = []\n",
    "for tokens in stemmed_docs:\n",
    "    all_stemmed_tokens.extend(tokens)\n",
    "\n",
    "# Create vocabulary (unique tokens)\n",
    "stemmed_vocabulary = sorted(list(set(all_stemmed_tokens)))\n",
    "\n",
    "print(f\"Total stemmed tokens (with repetition): {len(all_stemmed_tokens)}\")\n",
    "print(f\"Stemmed vocabulary size: {len(stemmed_vocabulary)}\")\n",
    "print(f\"\\nSample words from stemmed vocabulary: {stemmed_vocabulary[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5509bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming-based vocabulary size: 487\n",
      "\n",
      "Top 10 most frequent words:\n",
      "  1. day - 8 occurrences\n",
      "  2. go - 7 occurrences\n",
      "  3. know - 7 occurrences\n",
      "  4. work - 6 occurrences\n",
      "  5. back - 6 occurrences\n",
      "  6. miss - 6 occurrences\n",
      "  7. im - 6 occurrences\n",
      "  8. get - 5 occurrences\n",
      "  9. like - 5 occurrences\n",
      "  10. happi - 5 occurrences\n"
     ]
    }
   ],
   "source": [
    "# 8. Determine size and top 10 most frequent words (stemmed)\n",
    "\n",
    "# Count word frequencies\n",
    "stemmed_word_freq = Counter(all_stemmed_tokens)\n",
    "top_10_stemmed = stemmed_word_freq.most_common(10)\n",
    "\n",
    "print(f\"Stemming-based vocabulary size: {len(stemmed_vocabulary)}\")\n",
    "print(f\"\\nTop 10 most frequent words:\")\n",
    "for rank, (word, freq) in enumerate(top_10_stemmed, 1):\n",
    "    print(f\"  {rank}. {word} - {freq} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e79f5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed DTM shape: (100, 487)\n",
      "(Rows = 100 documents, Columns = 487 vocabulary)\n",
      "\n",
      "First document vector (first 20 dimensions): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Non-zero features in first document: 9\n"
     ]
    }
   ],
   "source": [
    "# 9. Generate Document-Term Matrix (DTM) for stemmed text\n",
    "\n",
    "# Create word to index mapping\n",
    "stemmed_word_to_idx = {word: idx for idx, word in enumerate(stemmed_vocabulary)}\n",
    "\n",
    "# Create DTM\n",
    "stemmed_dtm = []\n",
    "for tokens in stemmed_docs:\n",
    "    vector = [0] * len(stemmed_vocabulary)\n",
    "    for token in tokens:\n",
    "        if token in stemmed_word_to_idx:\n",
    "            vector[stemmed_word_to_idx[token]] += 1\n",
    "    stemmed_dtm.append(vector)\n",
    "\n",
    "stemmed_dtm = np.array(stemmed_dtm)\n",
    "\n",
    "print(f\"Stemmed DTM shape: {stemmed_dtm.shape}\")\n",
    "print(f\"(Rows = {stemmed_dtm.shape[0]} documents, Columns = {stemmed_dtm.shape[1]} vocabulary)\")\n",
    "print(f\"\\nFirst document vector (first 20 dimensions): {stemmed_dtm[0][:20]}\")\n",
    "print(f\"Non-zero features in first document: {np.count_nonzero(stemmed_dtm[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244c0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization applied to all 100 documents\n",
      "\n",
      "Example - Lemmatization transformations:\n",
      "  gone -> go\n",
      "  replaced -> replace\n"
     ]
    }
   ],
   "source": [
    "# 10. Apply lemmatization to the filtered tokens\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to all filtered documents\n",
    "lemmatized_docs = []\n",
    "for tokens in filtered_docs:\n",
    "    # Lemmatize as nouns first, then as verbs for better results\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='v') \n",
    "                        for word in tokens]\n",
    "    lemmatized_docs.append(lemmatized_tokens)\n",
    "\n",
    "print(f\"Lemmatization applied to all {len(lemmatized_docs)} documents\")\n",
    "print(f\"\\nExample - Lemmatization transformations:\")\n",
    "sample_tokens = filtered_docs[0][:10]\n",
    "sample_lemmatized = lemmatized_docs[0][:10]\n",
    "for original, lemmatized in zip(sample_tokens, sample_lemmatized):\n",
    "    if original != lemmatized:\n",
    "        print(f\"  {original} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fa61ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text column created\n",
      "\n",
      "Example - First document:\n",
      "Original: ['oooh, sunshine! A patch of sunshine! And it will be gone by the time I leave work and replaced with rain.  /vent', 'skype call with billie but my webcam dont work', 'Just had a great study time followed by a delicious Japanese meal with Arty! Now, trying to get back into the study mood', 'FML not having a car is prohibiting finding a job']...\n",
      "Lemmatized: 1    oooh sunshine patch sunshine go time leave wor...\n",
      "2                   skype call billie webcam dont work\n",
      "3    great study time follow delicious japanese mea...\n",
      "4                            fml car prohibit find job\n",
      "Name: lemmatized_text, dtype: object...\n"
     ]
    }
   ],
   "source": [
    "# 11. Create a new column with final preprocessed text after lemmatization\n",
    "\n",
    "# Join lemmatized tokens back into text\n",
    "df['lemmatized_text'] = [' '.join(tokens) for tokens in lemmatized_docs]\n",
    "\n",
    "print(f\"Lemmatized text column created\")\n",
    "print(f\"\\nExample - First document:\")\n",
    "print(f\"Original: {texts[0:4][:100]}...\")\n",
    "print(f\"Lemmatized: {df['lemmatized_text'].iloc[0:4][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1c9e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lemmatized tokens (with repetition): 701\n",
      "Lemmatized vocabulary size: 479\n",
      "\n",
      "Sample words from lemmatized vocabulary: ['ability', 'absoute', 'accept', 'account', 'act', 'ad', 'adapt', 'adobe', 'age', 'agency', 'ago', 'agree', 'ah', 'ahem', 'ahhh', 'already', 'alright', 'also', 'always', 'animal']\n"
     ]
    }
   ],
   "source": [
    "# 12. Construct the Bag-of-Words vocabulary using lemmatized text\n",
    "\n",
    "\n",
    "# Collect all lemmatized tokens\n",
    "all_lemmatized_tokens = []\n",
    "for tokens in lemmatized_docs:\n",
    "    all_lemmatized_tokens.extend(tokens)\n",
    "\n",
    "# Create vocabulary (unique tokens)\n",
    "lemmatized_vocabulary = sorted(list(set(all_lemmatized_tokens)))\n",
    "\n",
    "print(f\"Total lemmatized tokens (with repetition): {len(all_lemmatized_tokens)}\")\n",
    "print(f\"Lemmatized vocabulary size: {len(lemmatized_vocabulary)}\")\n",
    "print(f\"\\nSample words from lemmatized vocabulary: {lemmatized_vocabulary[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d349e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization-based vocabulary size: 479\n",
      "\n",
      "Top 10 most frequent words (lemmatized):\n",
      "  1. get - 9 occurrences\n",
      "  2. go - 8 occurrences\n",
      "  3. day - 8 occurrences\n",
      "  4. know - 8 occurrences\n",
      "  5. work - 6 occurrences\n",
      "  6. back - 6 occurrences\n",
      "  7. miss - 6 occurrences\n",
      "  8. make - 6 occurrences\n",
      "  9. im - 6 occurrences\n",
      "  10. like - 5 occurrences\n"
     ]
    }
   ],
   "source": [
    "# 13. Determine size and top 10 most frequent words (lemmatized)\n",
    "\n",
    "# Count word frequencies\n",
    "lemmatized_word_freq = Counter(all_lemmatized_tokens)\n",
    "top_10_lemmatized = lemmatized_word_freq.most_common(10)\n",
    "\n",
    "print(f\"Lemmatization-based vocabulary size: {len(lemmatized_vocabulary)}\")\n",
    "print(f\"\\nTop 10 most frequent words (lemmatized):\")\n",
    "for rank, (word, freq) in enumerate(top_10_lemmatized, 1):\n",
    "    print(f\"  {rank}. {word} - {freq} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc4404d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized DTM shape: (100, 479)\n",
      "(Rows = 100 documents, Columns = 479 vocabulary)\n",
      "\n",
      "First document vector (first 20 dimensions): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Non-zero features in first document: 9\n"
     ]
    }
   ],
   "source": [
    "# 14. Generate Document-Term Matrix (DTM) for lemmatized text\n",
    "\n",
    "# Create word to index mapping\n",
    "lemmatized_word_to_idx = {word: idx for idx, word in enumerate(lemmatized_vocabulary)}\n",
    "\n",
    "# Create DTM\n",
    "lemmatized_dtm = []\n",
    "for tokens in lemmatized_docs:\n",
    "    vector = [0] * len(lemmatized_vocabulary)\n",
    "    for token in tokens:\n",
    "        if token in lemmatized_word_to_idx:\n",
    "            vector[lemmatized_word_to_idx[token]] += 1\n",
    "    lemmatized_dtm.append(vector)\n",
    "\n",
    "lemmatized_dtm = np.array(lemmatized_dtm)\n",
    "\n",
    "print(f\"Lemmatized DTM shape: {lemmatized_dtm.shape}\")\n",
    "print(f\"(Rows = {lemmatized_dtm.shape[0]} documents, Columns = {lemmatized_dtm.shape[1]} vocabulary)\")\n",
    "print(f\"\\nFirst document vector (first 20 dimensions): {lemmatized_dtm[0][:20]}\")\n",
    "print(f\"Non-zero features in first document: {np.count_nonzero(lemmatized_dtm[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a6e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Document (Index 4):\n",
      "Original text:   lol since I got twitter a little while ago, it seems like loads of people setting up an account\n",
      "\n",
      "Stemmed text: lol sinc got twitter littl ago seem like load peopl set account\n",
      "Lemmatized text: lol since get twitter little ago seem like load people set account\n",
      "\n",
      "Stemming-based BoW Vector\n",
      "Vector length: 487\n",
      "Non-zero elements: 12\n",
      "Vector (first 30 dimensions): [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Non-zero features (word: count):\n",
      "  account : 1\n",
      "  ago : 1\n",
      "  got : 1\n",
      "  like : 1\n",
      "  littl : 1\n",
      "  load : 1\n",
      "  lol : 1\n",
      "  peopl : 1\n",
      "  seem : 1\n",
      "  set : 1\n",
      "  sinc : 1\n",
      "  twitter : 1\n",
      "\n",
      "Lemmatization-based BoW Vector\n",
      "Vector length: 479\n",
      "Non-zero elements: 12\n",
      "Vector (first 30 dimensions): [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Non-zero features (word: count):\n",
      "  account : 1\n",
      "  ago : 1\n",
      "  get : 1\n",
      "  like : 1\n",
      "  little : 1\n",
      "  load : 1\n",
      "  lol : 1\n",
      "  people : 1\n",
      "  seem : 1\n",
      "  set : 1\n",
      "  since : 1\n",
      "  twitter : 1\n"
     ]
    }
   ],
   "source": [
    "# 15. Select one document and display its BoW vector from both DTMs\n",
    "\n",
    "doc_idx = 4\n",
    "\n",
    "print(f\"Selected Document (Index {doc_idx}):\")\n",
    "print(f\"Original text: {texts[doc_idx]}\")\n",
    "print(f\"\\nStemmed text: {df['stemmed_text'].iloc[doc_idx]}\")\n",
    "print(f\"Lemmatized text: {df['lemmatized_text'].iloc[doc_idx]}\")\n",
    "\n",
    "print(f\"\\nStemming-based BoW Vector\")\n",
    "print(f\"Vector length: {len(stemmed_dtm[doc_idx])}\")\n",
    "print(f\"Non-zero elements: {np.count_nonzero(stemmed_dtm[doc_idx])}\")\n",
    "print(f\"Vector (first 30 dimensions): {stemmed_dtm[doc_idx][:30]}\")\n",
    "\n",
    "# Show non-zero features with their words\n",
    "print(f\"\\nNon-zero features (word: count):\")\n",
    "for idx, count in enumerate(stemmed_dtm[doc_idx]):\n",
    "    if count > 0:\n",
    "        print(f\"  {stemmed_vocabulary[idx]} : {count}\")\n",
    "\n",
    "print(f\"\\nLemmatization-based BoW Vector\")\n",
    "print(f\"Vector length: {len(lemmatized_dtm[doc_idx])}\")\n",
    "print(f\"Non-zero elements: {np.count_nonzero(lemmatized_dtm[doc_idx])}\")\n",
    "print(f\"Vector (first 30 dimensions): {lemmatized_dtm[doc_idx][:30]}\")\n",
    "\n",
    "# Show non-zero features with their words\n",
    "print(f\"\\nNon-zero features (word: count):\")\n",
    "for idx, count in enumerate(lemmatized_dtm[doc_idx]):\n",
    "    if count > 0:\n",
    "        print(f\"  {lemmatized_vocabulary[idx]} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e2b252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED VOCABULARY\n",
      "\n",
      "Words appearing in MOST documents (Top 10):\n",
      "  day appears in 8 documents (8.0%)\n",
      "  know appears in 7 documents (7.000000000000001%)\n",
      "  back appears in 6 documents (6.0%)\n",
      "  go appears in 6 documents (6.0%)\n",
      "  miss appears in 6 documents (6.0%)\n",
      "  work appears in 6 documents (6.0%)\n",
      "  get appears in 5 documents (5.0%)\n",
      "  happi appears in 5 documents (5.0%)\n",
      "  http appears in 5 documents (5.0%)\n",
      "  im appears in 5 documents (5.0%)\n",
      "\n",
      "Words appearing in VERY FEW documents (appearing in only 1 document):\n",
      "Total such words: 369\n",
      "Examples:\n",
      "  abil appears in 1 document\n",
      "  absout appears in 1 document\n",
      "  accept appears in 1 document\n",
      "  account appears in 1 document\n",
      "  act appears in 1 document\n",
      "  ad appears in 1 document\n",
      "  adapt appears in 1 document\n",
      "  adob appears in 1 document\n",
      "  age appears in 1 document\n",
      "  agenc appears in 1 document\n",
      "\n",
      "LEMMATIZED VOCABULARY\n",
      "\n",
      "Words appearing in MOST documents (Top 10):\n",
      "  get appears in 9 documents (9.0%)\n",
      "  day appears in 8 documents (8.0%)\n",
      "  know appears in 8 documents (8.0%)\n",
      "  go appears in 7 documents (7.000000000000001%)\n",
      "  back appears in 6 documents (6.0%)\n",
      "  miss appears in 6 documents (6.0%)\n",
      "  work appears in 6 documents (6.0%)\n",
      "  happy appears in 5 documents (5.0%)\n",
      "  http appears in 5 documents (5.0%)\n",
      "  im appears in 5 documents (5.0%)\n",
      "\n",
      "Words appearing in VERY FEW documents (appearing in only 1 document):\n",
      "Total such words: 361\n",
      "Examples:\n",
      "  ability appears in 1 document\n",
      "  absoute appears in 1 document\n",
      "  accept appears in 1 document\n",
      "  account appears in 1 document\n",
      "  act appears in 1 document\n",
      "  ad appears in 1 document\n",
      "  adapt appears in 1 document\n",
      "  adobe appears in 1 document\n",
      "  age appears in 1 document\n",
      "  agency appears in 1 document\n"
     ]
    }
   ],
   "source": [
    "# 16. Identify words appearing in most/few documents and discuss significance\n",
    "\n",
    "# Calculate document frequency for stemmed vocabulary\n",
    "stemmed_doc_freq = {}\n",
    "for word in stemmed_vocabulary:\n",
    "    doc_count = 0\n",
    "    for tokens in stemmed_docs:\n",
    "        if word in tokens:\n",
    "            doc_count += 1\n",
    "    stemmed_doc_freq[word] = doc_count\n",
    "\n",
    "# Calculate document frequency for lemmatized vocabulary\n",
    "lemmatized_doc_freq = {}\n",
    "for word in lemmatized_vocabulary:\n",
    "    doc_count = 0\n",
    "    for tokens in lemmatized_docs:\n",
    "        if word in tokens:\n",
    "            doc_count += 1\n",
    "    lemmatized_doc_freq[word] = doc_count\n",
    "\n",
    "# Find words appearing in most documents (stemmed)\n",
    "sorted_stemmed = sorted(stemmed_doc_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_stemmed = sorted_stemmed[:10]\n",
    "least_common_stemmed = [item for item in sorted_stemmed if item[1] == 1][:10]\n",
    "\n",
    "# Find words appearing in most documents (lemmatized)\n",
    "sorted_lemmatized = sorted(lemmatized_doc_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_lemmatized = sorted_lemmatized[:10]\n",
    "least_common_lemmatized = [item for item in sorted_lemmatized if item[1] == 1][:10]\n",
    "\n",
    "print(\"STEMMED VOCABULARY\")\n",
    "print(f\"\\nWords appearing in MOST documents (Top 10):\")\n",
    "for word, doc_count in most_common_stemmed:\n",
    "    percentage = (doc_count / len(texts)) * 100\n",
    "    print(f\"  {word} appears in {doc_count} documents ({percentage}%)\")\n",
    "\n",
    "print(f\"\\nWords appearing in VERY FEW documents (appearing in only 1 document):\")\n",
    "print(f\"Total such words: {len([item for item in sorted_stemmed if item[1] == 1])}\")\n",
    "print(f\"Examples:\")\n",
    "for word, doc_count in least_common_stemmed:\n",
    "    print(f\"  {word} appears in {doc_count} document\")\n",
    "\n",
    "print(\"\\nLEMMATIZED VOCABULARY\")\n",
    "print(f\"\\nWords appearing in MOST documents (Top 10):\")\n",
    "for word, doc_count in most_common_lemmatized:\n",
    "    percentage = (doc_count / len(texts)) * 100\n",
    "    print(f\"  {word} appears in {doc_count} documents ({percentage}%)\")\n",
    "\n",
    "print(f\"\\nWords appearing in VERY FEW documents (appearing in only 1 document):\")\n",
    "print(f\"Total such words: {len([item for item in sorted_lemmatized if item[1] == 1])}\")\n",
    "print(f\"Examples:\")\n",
    "for word, doc_count in least_common_lemmatized:\n",
    "    print(f\"  {word} appears in {doc_count} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64d106ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "high frequency words (appearing in most documents):\n",
      "stemmed: 'day' (8 docs), 'go' (7 docs), 'know' (7 docs)\n",
      "lemmatized: 'get' (9 docs), 'day' (8 docs), 'know' (8 docs)\n",
      "\n",
      "what this means:\n",
      "- even top words appear in less than 10% of documents\n",
      "- these are general action verbs: get, go, know, work, miss\n",
      "- they dont carry specific sentiment (neutral words)\n",
      "- appear in positive, negative, and neutral reviews equally\n",
      "- not very useful for distinguishing between sentiment classes\n",
      "- better for identifying general topics rather than sentiment\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nhigh frequency words (appearing in most documents):\")\n",
    "print(\"stemmed: 'day' (8 docs), 'go' (7 docs), 'know' (7 docs)\")\n",
    "print(\"lemmatized: 'get' (9 docs), 'day' (8 docs), 'know' (8 docs)\")\n",
    "print(\"\\nwhat this means:\")\n",
    "print(\"- even top words appear in less than 10% of documents\")\n",
    "print(\"- these are general action verbs: get, go, know, work, miss\")\n",
    "print(\"- they dont carry specific sentiment (neutral words)\")\n",
    "print(\"- appear in positive, negative, and neutral reviews equally\")\n",
    "print(\"- not very useful for distinguishing between sentiment classes\")\n",
    "print(\"- better for identifying general topics rather than sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb364d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for sentiment classification:\n",
      "ideal features would be:\n",
      "- words appearing in 20-50% of documents\n",
      "- sentiment-bearing words: love, hate, great, terrible\n",
      "- currently missing because dataset is small (100 docs)\n",
      "- most words are too rare to be reliable features\n",
      "- model might struggle due to high sparsity\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nfor sentiment classification:\")\n",
    "print(\"ideal features would be:\")\n",
    "print(\"- words appearing in 20-50% of documents\")\n",
    "print(\"- sentiment-bearing words: love, hate, great, terrible\")\n",
    "print(\"- currently missing because dataset is small (100 docs)\")\n",
    "print(\"- most words are too rare to be reliable features\")\n",
    "print(\"- model might struggle due to high sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "badc790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the high proportion of rare words (75%) indicates:\n",
      "1. very diverse vocabulary across documents\n",
      "2. small dataset with limited word repetition\n",
      "3. need for more data or different feature engineering\n",
      "4. tf-idf might help reduce impact of rare words\n",
      "5. lemmatization slightly better than stemming here\n"
     ]
    }
   ],
   "source": [
    "print(\"the high proportion of rare words (75%) indicates:\")\n",
    "print(\"1. very diverse vocabulary across documents\")\n",
    "print(\"2. small dataset with limited word repetition\")\n",
    "print(\"3. need for more data or different feature engineering\")\n",
    "print(\"4. tf-idf might help reduce impact of rare words\")\n",
    "print(\"5. lemmatization slightly better than stemming here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b2197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
