{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5101caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9f2323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/navya/nlp/.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/navya/nlp/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m694.8 kB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m547.7 kB/s\u001b[0m  \u001b[33m0:00:29\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.4.0 pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset\\\\test.csv', encoding='latin1')\n",
    " \n",
    "# removing all cols except 'text' and 'sentiment'\n",
    "df = df.drop(columns=[col for col in df.columns if col not in ['text', 'sentiment']])\n",
    "\n",
    "df = df.dropna()\n",
    "#rdropping the first hundred rows and retaining 100 to 200 rows and dropping the rest\n",
    "\n",
    "df = df.drop(df.index[0:100])\n",
    "df = df.drop(df.index[100:])\n",
    "\n",
    "#make it startfrom 0 again\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#call sentiment as label\n",
    "df.rename(columns={'sentiment': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d049b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  oooh, sunshine! A patch of sunshine! And it wi...   neutral\n",
      "1     skype call with billie but my webcam dont work   neutral\n",
      "2  Just had a great study time followed by a deli...  positive\n",
      "Total number of rows: 100\n",
      "Total number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "#Display the first three records of the dataset and print the total number of rows and columns\n",
    "print(df.head(3))\n",
    "print(\"Total number of rows:\", df.shape[0])\n",
    "print(\"Total number of columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb55b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     object\n",
      "label    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Print the column names and data types of each column. \n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5665a67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check whether the dataset contains any missing values and display the count of missing values per column. \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ead1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 3\n"
     ]
    }
   ],
   "source": [
    "#Find the number of unique labels present in the dataset.\n",
    "print(\"Number of unique labels:\", df['label'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b20bd28",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Display the count of records for each label.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "#Display the count of records for each label.\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label\n",
      "3   FML not having a car is prohibiting finding a job  negative\n",
      "9    yeah i need his fricken cell number ive tried...  negative\n",
      "14                               bahah sadly I am not  negative\n",
      "18   thats rough. hubby had knee surgery but it di...  negative\n",
      "35                               Now I have a sunburn  negative\n",
      "40  ok so this eye doctor guy is taking forever an...  negative\n",
      "41     Im Slowing on My Tweets..Cuase I Lost My Phone  negative\n",
      "47   It`s gonna be hot here today too. today is Ry...  negative\n",
      "49  Oh, **** me. I`ve just returned from the Super...  negative\n",
      "56  is pretty dang tired. but chambers class is fo...  negative\n",
      "57         i have to poop...   is hogging the shitter  negative\n",
      "69                 hate working when its sunny,boohoo  negative\n",
      "72  Wondering if i cld make things any worse than ...  negative\n",
      "74  Twitter has spoiled all the fun. Frustratingly...  negative\n",
      "79   many! mine included. ahem. shld have known be...  negative\n",
      "81                        I will miss the soccer moms  negative\n",
      "87  just watched yes man. bahaha. that movie is me...  negative\n",
      "88   Sadly I think I know exactly were you put it-...  negative\n",
      "89   Too much? No, not unless one of the pillows e...  negative\n",
      "91  Wow that was fast...next up Soderling or Ferre...  negative\n"
     ]
    }
   ],
   "source": [
    "#Filter and display all rows where the label is \"Negative\"\n",
    "negative_df = df[df['label'] == 'negative']\n",
    "print(negative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d869c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length\n",
      "0   oooh, sunshine! A patch of sunshine! And it wi...   neutral          112\n",
      "1      skype call with billie but my webcam dont work   neutral           46\n",
      "2   Just had a great study time followed by a deli...  positive          120\n",
      "3   FML not having a car is prohibiting finding a job  negative           49\n",
      "4     lol since I got twitter a little while ago, ...   neutral           97\n",
      "..                                                ...       ...          ...\n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128\n",
      "96                ???    ?????? - http://bit.ly/nAcK2   neutral           36\n",
      "97  Waiting for the last movie to finish then am t...   neutral          128\n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70\n",
      "99                       Must head back to the office   neutral           28\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create a new column that stores the number of characters present in each text.\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03a9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "0   oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "1      skype call with billie but my webcam dont work   neutral           46   \n",
      "2   Just had a great study time followed by a deli...  positive          120   \n",
      "3   FML not having a car is prohibiting finding a job  negative           49   \n",
      "4     lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                ...       ...          ...   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "96                ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "97  Waiting for the last movie to finish then am t...   neutral          128   \n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "99                       Must head back to the office   neutral           28   \n",
      "\n",
      "    word_count  \n",
      "0           22  \n",
      "1            9  \n",
      "2           23  \n",
      "3           10  \n",
      "4           19  \n",
      "..         ...  \n",
      "95          26  \n",
      "96           4  \n",
      "97          24  \n",
      "98          11  \n",
      "99           6  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create another column that stores the number of words in each text. \n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0486dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "0   oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "1      skype call with billie but my webcam dont work   neutral           46   \n",
      "2   Just had a great study time followed by a deli...  positive          120   \n",
      "3   FML not having a car is prohibiting finding a job  negative           49   \n",
      "4     lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                ...       ...          ...   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "96                ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "97  Waiting for the last movie to finish then am t...   neutral          128   \n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "99                       Must head back to the office   neutral           28   \n",
      "\n",
      "    word_count                                         text_lower  \n",
      "0           22  oooh, sunshine! a patch of sunshine! and it wi...  \n",
      "1            9     skype call with billie but my webcam dont work  \n",
      "2           23  just had a great study time followed by a deli...  \n",
      "3           10  fml not having a car is prohibiting finding a job  \n",
      "4           19    lol since i got twitter a little while ago, ...  \n",
      "..         ...                                                ...  \n",
      "95          26  'i don`t like monday, i wish it were sunday, c...  \n",
      "96           4                ???    ?????? - http://bit.ly/nack2  \n",
      "97          24  waiting for the last movie to finish then am t...  \n",
      "98          11  happy #starwarsday. may the 4th be with you!  ...  \n",
      "99           6                       must head back to the office  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Convert all text entries to lowercase and store the result in a new column without modifying the original text column. \n",
    "df['text_lower'] = df['text'].apply(lambda x: x.lower())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcee6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "0   oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "1      skype call with billie but my webcam dont work   neutral           46   \n",
      "2   Just had a great study time followed by a deli...  positive          120   \n",
      "3   FML not having a car is prohibiting finding a job  negative           49   \n",
      "4     lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                ...       ...          ...   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "96                ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "97  Waiting for the last movie to finish then am t...   neutral          128   \n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "99                       Must head back to the office   neutral           28   \n",
      "\n",
      "    word_count                                         text_lower  \n",
      "0           22  oooh, sunshine! a patch of sunshine! and it wi...  \n",
      "1            9     skype call with billie but my webcam dont work  \n",
      "2           23  just had a great study time followed by a deli...  \n",
      "3           10  fml not having a car is prohibiting finding a job  \n",
      "4           19    lol since i got twitter a little while ago, ...  \n",
      "..         ...                                                ...  \n",
      "95          26  'i don`t like monday, i wish it were sunday, c...  \n",
      "96           4                ???    ?????? - http://bit.ly/nack2  \n",
      "97          24  waiting for the last movie to finish then am t...  \n",
      "98          11  happy #starwarsday. may the 4th be with you!  ...  \n",
      "99           6                       must head back to the office  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Standardize the label column by converting all labels to lowercase.\n",
    "df['label'] = df['label'].apply(lambda x: x.lower())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31864bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "0   oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "1      skype call with billie but my webcam dont work   neutral           46   \n",
      "2   Just had a great study time followed by a deli...  positive          120   \n",
      "3   FML not having a car is prohibiting finding a job  negative           49   \n",
      "4     lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                ...       ...          ...   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "96                ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "97  Waiting for the last movie to finish then am t...   neutral          128   \n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "99                       Must head back to the office   neutral           28   \n",
      "\n",
      "    word_count                                         text_lower  \\\n",
      "0           22  oooh, sunshine! a patch of sunshine! and it wi...   \n",
      "1            9     skype call with billie but my webcam dont work   \n",
      "2           23  just had a great study time followed by a deli...   \n",
      "3           10  fml not having a car is prohibiting finding a job   \n",
      "4           19    lol since i got twitter a little while ago, ...   \n",
      "..         ...                                                ...   \n",
      "95          26  'i don`t like monday, i wish it were sunday, c...   \n",
      "96           4                ???    ?????? - http://bit.ly/nack2   \n",
      "97          24  waiting for the last movie to finish then am t...   \n",
      "98          11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "99           6                       must head back to the office   \n",
      "\n",
      "    label_encoded  \n",
      "0               0  \n",
      "1               0  \n",
      "2               1  \n",
      "3              -1  \n",
      "4               0  \n",
      "..            ...  \n",
      "95              0  \n",
      "96              0  \n",
      "97              0  \n",
      "98              1  \n",
      "99              0  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create a new column in which the labels are encoded as follows: positive → 1, neutral → 0, negative → −1.\n",
    "label_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "df['label_encoded'] = df['label'].map(label_mapping)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c88c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "58  YES! Im down to 50% full on my dvr  i was at 9...   neutral          124   \n",
      "38  I <3 Rupert Grint......& of the boy, Ive accep...   neutral          137   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "47   It`s gonna be hot here today too. today is Ry...  negative          124   \n",
      "18   thats rough. hubby had knee surgery but it di...  negative          123   \n",
      "..                                                ...       ...          ...   \n",
      "39                                    love your books  positive           16   \n",
      "70                is ONLiNE  http://plurk.com/p/stjdg   neutral           35   \n",
      "46                                  Happy Mothers Day  positive           18   \n",
      "30                                           Welcome!  positive            9   \n",
      "94                                              woot!  positive            6   \n",
      "\n",
      "    word_count                                         text_lower  \\\n",
      "58          31  yes! im down to 50% full on my dvr  i was at 9...   \n",
      "38          28  i <3 rupert grint......& of the boy, ive accep...   \n",
      "95          26  'i don`t like monday, i wish it were sunday, c...   \n",
      "47          26   it`s gonna be hot here today too. today is ry...   \n",
      "18          25   thats rough. hubby had knee surgery but it di...   \n",
      "..         ...                                                ...   \n",
      "39           3                                    love your books   \n",
      "70           3                is online  http://plurk.com/p/stjdg   \n",
      "46           3                                  happy mothers day   \n",
      "30           1                                           welcome!   \n",
      "94           1                                              woot!   \n",
      "\n",
      "    label_encoded  \n",
      "58              0  \n",
      "38              0  \n",
      "95              0  \n",
      "47             -1  \n",
      "18             -1  \n",
      "..            ...  \n",
      "39              1  \n",
      "70              0  \n",
      "46              1  \n",
      "30              1  \n",
      "94              1  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Sort the DataFrame in descending order based on the word count column. \n",
    "df_sorted = df.sort_values(by='word_count', ascending=False)\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a586b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text    label  text_length  \\\n",
      "58  YES! Im down to 50% full on my dvr  i was at 9...  neutral          124   \n",
      "38  I <3 Rupert Grint......& of the boy, Ive accep...  neutral          137   \n",
      "95  'I don`t like Monday, i wish it were sunday, c...  neutral          128   \n",
      "\n",
      "    word_count                                         text_lower  \\\n",
      "58          31  yes! im down to 50% full on my dvr  i was at 9...   \n",
      "38          28  i <3 rupert grint......& of the boy, ive accep...   \n",
      "95          26  'i don`t like monday, i wish it were sunday, c...   \n",
      "\n",
      "    label_encoded  \n",
      "58              0  \n",
      "38              0  \n",
      "95              0  \n"
     ]
    }
   ],
   "source": [
    "#Display the top three longest text entries based on word count.\n",
    "print(df_sorted.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bcf22cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  text_length  \\\n",
      "2   Just had a great study time followed by a deli...  positive          120   \n",
      "15   i always have those for my Champions League p...  positive           65   \n",
      "16   well, not all. just stay away from those kind...  positive          128   \n",
      "20  Playin City of Villains, wishin my buddies wer...  positive           67   \n",
      "21   yep - three things: a good haircut, the abili...  positive          109   \n",
      "24   I liked it.  Did you record it yourself?  If ...  positive           80   \n",
      "26   Folks thought it was hilarious when I told th...  positive          124   \n",
      "34  just got home from a nice party, just not tire...  positive           51   \n",
      "36  I have the Job  this is a nice day it can not ...  positive           55   \n",
      "37            I wish I could go to #BEA this weekend.  positive           39   \n",
      "43   why would they take a photo with stuffed anim...  positive           71   \n",
      "45  tikcets are only Ã¯Â¿Â½91...each...BUT I SO WA...  positive           54   \n",
      "51  D-group Saturday`s, with a little bit of footb...  positive          134   \n",
      "52  just set up a new computer again. 15min. I lov...  positive           61   \n",
      "53  Ahhh my mommie got me new sheets for my bed  s...  positive           53   \n",
      "59   Happy muthath`s day to all moms!!! I salute you!  positive           48   \n",
      "61   Aren`t you suppossed to support the local eco...  positive           88   \n",
      "66   E.L.O.  wow, brings back so many happy memori...  positive          119   \n",
      "67   that`s a very cute picture ... but you don`t ...  positive          127   \n",
      "68   oh i love sunday mornings like this - mum jus...  positive           76   \n",
      "78  Going to bed after a great night with a friend...  positive          116   \n",
      "80  it`s nice to leave the office when the sun is ...  positive           54   \n",
      "93   glad to know that the ad display problem was ...  positive           90   \n",
      "98  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "\n",
      "    word_count                                         text_lower  \\\n",
      "2           23  just had a great study time followed by a deli...   \n",
      "15          11   i always have those for my champions league p...   \n",
      "16          23   well, not all. just stay away from those kind...   \n",
      "20          11  playin city of villains, wishin my buddies wer...   \n",
      "21          20   yep - three things: a good haircut, the abili...   \n",
      "24          16   i liked it.  did you record it yourself?  if ...   \n",
      "26          23   folks thought it was hilarious when i told th...   \n",
      "34          11  just got home from a nice party, just not tire...   \n",
      "36          14  i have the job  this is a nice day it can not ...   \n",
      "37           9            i wish i could go to #bea this weekend.   \n",
      "43          12   why would they take a photo with stuffed anim...   \n",
      "45           9  tikcets are only ã¯â¿â½91...each...but i so wa...   \n",
      "51          23  d-group saturday`s, with a little bit of footb...   \n",
      "52          12  just set up a new computer again. 15min. i lov...   \n",
      "53          12  ahhh my mommie got me new sheets for my bed  s...   \n",
      "59           9   happy muthath`s day to all moms!!! i salute you!   \n",
      "61          16   aren`t you suppossed to support the local eco...   \n",
      "66          22   e.l.o.  wow, brings back so many happy memori...   \n",
      "67          25   that`s a very cute picture ... but you don`t ...   \n",
      "68          15   oh i love sunday mornings like this - mum jus...   \n",
      "78          23  going to bed after a great night with a friend...   \n",
      "80          12  it`s nice to leave the office when the sun is ...   \n",
      "93          18   glad to know that the ad display problem was ...   \n",
      "98          11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "\n",
      "    label_encoded  \n",
      "2               1  \n",
      "15              1  \n",
      "16              1  \n",
      "20              1  \n",
      "21              1  \n",
      "24              1  \n",
      "26              1  \n",
      "34              1  \n",
      "36              1  \n",
      "37              1  \n",
      "43              1  \n",
      "45              1  \n",
      "51              1  \n",
      "52              1  \n",
      "53              1  \n",
      "59              1  \n",
      "61              1  \n",
      "66              1  \n",
      "67              1  \n",
      "68              1  \n",
      "78              1  \n",
      "80              1  \n",
      "93              1  \n",
      "98              1  \n"
     ]
    }
   ],
   "source": [
    "#Filter and display all texts that contain more than six words and belong to the \"positive\" label.\n",
    "positive_more_than_6_words = df[(df['word_count'] > 6) & (df['label'] == 'positive')]\n",
    "print(positive_more_than_6_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cdb03eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "negative    13.800000\n",
       "neutral     13.955556\n",
       "positive    12.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group the data by label and compute the average word count for each group. \n",
    "df.groupby('label')['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66d7869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label with highest average word count: neutral\n",
      "Average word count: 13.96\n",
      "\n",
      "All labels with their average word counts:\n",
      "label\n",
      "negative    13.800000\n",
      "neutral     13.955556\n",
      "positive    12.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Identify the label that has the highest average word count.\n",
    "avg_word_count = df.groupby('label')['word_count'].mean()\n",
    "label_highest_avg = avg_word_count.idxmax()\n",
    "print(f\"Label with highest average word count: {label_highest_avg}\")\n",
    "print(f\"Average word count: {avg_word_count[label_highest_avg]:.2f}\")\n",
    "print(\"\\nAll labels with their average word counts:\")\n",
    "print(avg_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6202869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "neutral     4\n",
      "positive    8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Count the number of text entries with fewer than five words for each label.\n",
    "fewer_than_5_words = df[df['word_count'] < 5].groupby('label').size()\n",
    "print(fewer_than_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a81111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     label  text_length  \\\n",
      "1    oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "2       skype call with billie but my webcam dont work   neutral           46   \n",
      "3    Just had a great study time followed by a deli...  positive          120   \n",
      "4    FML not having a car is prohibiting finding a job  negative           49   \n",
      "5      lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                 ...       ...          ...   \n",
      "96   'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "97                 ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "98   Waiting for the last movie to finish then am t...   neutral          128   \n",
      "99   Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "100                       Must head back to the office   neutral           28   \n",
      "\n",
      "     word_count                                         text_lower  \\\n",
      "1            22  oooh, sunshine! a patch of sunshine! and it wi...   \n",
      "2             9     skype call with billie but my webcam dont work   \n",
      "3            23  just had a great study time followed by a deli...   \n",
      "4            10  fml not having a car is prohibiting finding a job   \n",
      "5            19    lol since i got twitter a little while ago, ...   \n",
      "..          ...                                                ...   \n",
      "96           26  'i don`t like monday, i wish it were sunday, c...   \n",
      "97            4                ???    ?????? - http://bit.ly/nack2   \n",
      "98           24  waiting for the last movie to finish then am t...   \n",
      "99           11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "100           6                       must head back to the office   \n",
      "\n",
      "     label_encoded  \n",
      "1                0  \n",
      "2                0  \n",
      "3                1  \n",
      "4               -1  \n",
      "5                0  \n",
      "..             ...  \n",
      "96               0  \n",
      "97               0  \n",
      "98               0  \n",
      "99               1  \n",
      "100              0  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Change the DataFrame index to start from 1 instead of 0.  \n",
    "df.index = range(1, len(df) + 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f40ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           review_text sentiment  text_length  \\\n",
      "1    oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "2       skype call with billie but my webcam dont work   neutral           46   \n",
      "3    Just had a great study time followed by a deli...  positive          120   \n",
      "4    FML not having a car is prohibiting finding a job  negative           49   \n",
      "5      lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                 ...       ...          ...   \n",
      "96   'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "97                 ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "98   Waiting for the last movie to finish then am t...   neutral          128   \n",
      "99   Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "100                       Must head back to the office   neutral           28   \n",
      "\n",
      "     word_count                                         text_lower  \\\n",
      "1            22  oooh, sunshine! a patch of sunshine! and it wi...   \n",
      "2             9     skype call with billie but my webcam dont work   \n",
      "3            23  just had a great study time followed by a deli...   \n",
      "4            10  fml not having a car is prohibiting finding a job   \n",
      "5            19    lol since i got twitter a little while ago, ...   \n",
      "..          ...                                                ...   \n",
      "96           26  'i don`t like monday, i wish it were sunday, c...   \n",
      "97            4                ???    ?????? - http://bit.ly/nack2   \n",
      "98           24  waiting for the last movie to finish then am t...   \n",
      "99           11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "100           6                       must head back to the office   \n",
      "\n",
      "     label_encoded  \n",
      "1                0  \n",
      "2                0  \n",
      "3                1  \n",
      "4               -1  \n",
      "5                0  \n",
      "..             ...  \n",
      "96               0  \n",
      "97               0  \n",
      "98               0  \n",
      "99               1  \n",
      "100              0  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Rename the columns text to review_text and label to sentiment.\n",
    "df.rename(columns={'text': 'review_text', 'label': 'sentiment'}, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bffabceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2378139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed record at index 14 from 'positive' to 'neutral'\n",
      "                                           review_text sentiment  text_length  \\\n",
      "1    oooh, sunshine! A patch of sunshine! And it wi...   neutral          112   \n",
      "2       skype call with billie but my webcam dont work   neutral           46   \n",
      "3    Just had a great study time followed by a deli...   neutral          120   \n",
      "4    FML not having a car is prohibiting finding a job  negative           49   \n",
      "5      lol since I got twitter a little while ago, ...   neutral           97   \n",
      "..                                                 ...       ...          ...   \n",
      "96   'I don`t like Monday, i wish it were sunday, c...   neutral          128   \n",
      "97                 ???    ?????? - http://bit.ly/nAcK2   neutral           36   \n",
      "98   Waiting for the last movie to finish then am t...   neutral          128   \n",
      "99   Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "100                       Must head back to the office   neutral           28   \n",
      "\n",
      "     word_count                                         text_lower  \\\n",
      "1            22  oooh, sunshine! a patch of sunshine! and it wi...   \n",
      "2             9     skype call with billie but my webcam dont work   \n",
      "3            23  just had a great study time followed by a deli...   \n",
      "4            10  fml not having a car is prohibiting finding a job   \n",
      "5            19    lol since i got twitter a little while ago, ...   \n",
      "..          ...                                                ...   \n",
      "96           26  'i don`t like monday, i wish it were sunday, c...   \n",
      "97            4                ???    ?????? - http://bit.ly/nack2   \n",
      "98           24  waiting for the last movie to finish then am t...   \n",
      "99           11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "100           6                       must head back to the office   \n",
      "\n",
      "     label_encoded  \n",
      "1                0  \n",
      "2                0  \n",
      "3                1  \n",
      "4               -1  \n",
      "5                0  \n",
      "..             ...  \n",
      "96               0  \n",
      "97               0  \n",
      "98               0  \n",
      "99               1  \n",
      "100              0  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find a record with 'positive' sentiment and change it to 'neutral'\n",
    "positive_indices = df[df['sentiment'] == 'positive'].index\n",
    "if len(positive_indices) > 0:\n",
    "    index_to_change = positive_indices[0]\n",
    "    df.loc[index_to_change, 'sentiment'] = 'neutral'\n",
    "    print(f\"Changed record at index {index_to_change} from 'positive' to 'neutral'\")\n",
    "else:\n",
    "    print(\"No records with 'positive' sentiment found\")\n",
    "    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94bcce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with only positive sentiment records:\n",
      "                                          review_text sentiment  text_length  \\\n",
      "16   i always have those for my Champions League p...  positive           65   \n",
      "17   well, not all. just stay away from those kind...  positive          128   \n",
      "21  Playin City of Villains, wishin my buddies wer...  positive           67   \n",
      "22   yep - three things: a good haircut, the abili...  positive          109   \n",
      "25   I liked it.  Did you record it yourself?  If ...  positive           80   \n",
      "27   Folks thought it was hilarious when I told th...  positive          124   \n",
      "28                            you`re the absoute best  positive           24   \n",
      "31                                           Welcome!  positive            9   \n",
      "35  just got home from a nice party, just not tire...  positive           51   \n",
      "37  I have the Job  this is a nice day it can not ...  positive           55   \n",
      "38            I wish I could go to #BEA this weekend.  positive           39   \n",
      "40                                    love your books  positive           16   \n",
      "43                      well thank your phone for me.  positive           30   \n",
      "44   why would they take a photo with stuffed anim...  positive           71   \n",
      "46  tikcets are only Ã¯Â¿Â½91...each...BUT I SO WA...  positive           54   \n",
      "47                                  Happy Mothers Day  positive           18   \n",
      "52  D-group Saturday`s, with a little bit of footb...  positive          134   \n",
      "53  just set up a new computer again. 15min. I lov...  positive           61   \n",
      "54  Ahhh my mommie got me new sheets for my bed  s...  positive           53   \n",
      "60   Happy muthath`s day to all moms!!! I salute you!  positive           48   \n",
      "61                                Hahaha! Alright  ..  positive           19   \n",
      "62   Aren`t you suppossed to support the local eco...  positive           88   \n",
      "63                              good to know   thanks  positive           22   \n",
      "67   E.L.O.  wow, brings back so many happy memori...  positive          119   \n",
      "68   that`s a very cute picture ... but you don`t ...  positive          127   \n",
      "69   oh i love sunday mornings like this - mum jus...  positive           76   \n",
      "76                    _chick haha. I completely agree  positive           31   \n",
      "79  Going to bed after a great night with a friend...  positive          116   \n",
      "81  it`s nice to leave the office when the sun is ...  positive           54   \n",
      "94   glad to know that the ad display problem was ...  positive           90   \n",
      "95                                              woot!  positive            6   \n",
      "99  Happy #StarWarsDay. May the 4th be with you!  ...  positive           70   \n",
      "\n",
      "    word_count                                         text_lower  \\\n",
      "16          11   i always have those for my champions league p...   \n",
      "17          23   well, not all. just stay away from those kind...   \n",
      "21          11  playin city of villains, wishin my buddies wer...   \n",
      "22          20   yep - three things: a good haircut, the abili...   \n",
      "25          16   i liked it.  did you record it yourself?  if ...   \n",
      "27          23   folks thought it was hilarious when i told th...   \n",
      "28           4                            you`re the absoute best   \n",
      "31           1                                           welcome!   \n",
      "35          11  just got home from a nice party, just not tire...   \n",
      "37          14  i have the job  this is a nice day it can not ...   \n",
      "38           9            i wish i could go to #bea this weekend.   \n",
      "40           3                                    love your books   \n",
      "43           6                      well thank your phone for me.   \n",
      "44          12   why would they take a photo with stuffed anim...   \n",
      "46           9  tikcets are only ã¯â¿â½91...each...but i so wa...   \n",
      "47           3                                  happy mothers day   \n",
      "52          23  d-group saturday`s, with a little bit of footb...   \n",
      "53          12  just set up a new computer again. 15min. i lov...   \n",
      "54          12  ahhh my mommie got me new sheets for my bed  s...   \n",
      "60           9   happy muthath`s day to all moms!!! i salute you!   \n",
      "61           3                                hahaha! alright  ..   \n",
      "62          16   aren`t you suppossed to support the local eco...   \n",
      "63           4                              good to know   thanks   \n",
      "67          22   e.l.o.  wow, brings back so many happy memori...   \n",
      "68          25   that`s a very cute picture ... but you don`t ...   \n",
      "69          15   oh i love sunday mornings like this - mum jus...   \n",
      "76           5                    _chick haha. i completely agree   \n",
      "79          23  going to bed after a great night with a friend...   \n",
      "81          12  it`s nice to leave the office when the sun is ...   \n",
      "94          18   glad to know that the ad display problem was ...   \n",
      "95           1                                              woot!   \n",
      "99          11  happy #starwarsday. may the 4th be with you!  ...   \n",
      "\n",
      "    label_encoded  \n",
      "16              1  \n",
      "17              1  \n",
      "21              1  \n",
      "22              1  \n",
      "25              1  \n",
      "27              1  \n",
      "28              1  \n",
      "31              1  \n",
      "35              1  \n",
      "37              1  \n",
      "38              1  \n",
      "40              1  \n",
      "43              1  \n",
      "44              1  \n",
      "46              1  \n",
      "47              1  \n",
      "52              1  \n",
      "53              1  \n",
      "54              1  \n",
      "60              1  \n",
      "61              1  \n",
      "62              1  \n",
      "63              1  \n",
      "67              1  \n",
      "68              1  \n",
      "69              1  \n",
      "76              1  \n",
      "79              1  \n",
      "81              1  \n",
      "94              1  \n",
      "95              1  \n",
      "99              1  \n",
      "\n",
      "Total records with positive sentiment: 32\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with only positive sentiment records\n",
    "positive_df = df[df['sentiment'] == 'positive']\n",
    "print(\"DataFrame with only positive sentiment records:\")\n",
    "print(positive_df)\n",
    "print(f\"\\nTotal records with positive sentiment: {len(positive_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'cleaned_data.csv'\n",
      "File contains 100 records and 6 columns\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned and updated DataFrame to a new CSV file\n",
    "output_file = 'dataset\\\\cleaned_data.csv'\n",
    "df.to_csv(output_file, index=True, encoding='utf-8')\n",
    "print(f\"DataFrame saved to '{output_file}'\")\n",
    "print(f\"File contains {len(df)} records and {len(df.columns)} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
